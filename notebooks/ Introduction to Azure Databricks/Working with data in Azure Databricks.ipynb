{"cells":[{"cell_type":"markdown","source":["# Working with data in Azure Databricks\n\n**Technical Accomplishments:**\n- viewing available tables\n- loading table data in dataframes\n- loading file/dbfs data in dataframes\n- using spark for simple queries\n- using spark to show the data and its structure\n- using spark for complex queries\n- using Databricks' `display` for custom visualisations"],"metadata":{}},{"cell_type":"markdown","source":["## Attach notebook to your cluster\nBefore executing any cells in the notebook, you need to attach it to your cluster. Make sure that the cluster is running.\n\nIn the notebook's toolbar, select the drop down arrow next to Detached, and then select your cluster under Attach to."],"metadata":{}},{"cell_type":"markdown","source":["## About Spark DataFrames\n\nSpark DataFrames are distributed collections of data, organized into rows and columns, similar to traditional SQL tables.\n\nA DataFrame can be operated on using relational transformations, through the Spark SQL API, which is available in Scala, Java, Python, and R.\n\nWe will use Python in our notebook. \n\nWe often refer to DataFrame variables using `df`."],"metadata":{}},{"cell_type":"markdown","source":["## Loading data into dataframes"],"metadata":{}},{"cell_type":"markdown","source":["#### View available data\n\nTo check the data available in our Databricks environment we can use the `%sql` magic and query our tables:"],"metadata":{}},{"cell_type":"code","source":["%sql\n\nselect * from nyc_taxi;"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["#### Reading data from our tables\n\nUsing Spark, we can read data into dataframes. \n\nIt is important to note that spark has read/write support for a widely set of formats. \nIt can use\n* csv\n* json\n* parquet\n* orc\n* avro\n* hive tables\n* jdbc\n\nWe can read our data from the tables (since we already imported the initial csv as Databricks tables)."],"metadata":{}},{"cell_type":"code","source":["df = spark.sql(\"SELECT * FROM nyc_taxi\")\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["#### Reading data from the DBFS\n\nWe can also read the data from the original files we've uploaded; or indeed from any other file available in the DBFS. \n\nThe code is the same regardless of whether a file is local or in mounted remote storage that was mounted, thanks to DBFS mountpoints"],"metadata":{}},{"cell_type":"code","source":["df = spark.read.csv('dbfs:/FileStore/tables/nyc_taxi.csv', header=True, inferSchema=True)\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["#### DataFrame size\n\nUse `count` to determine how many rows of data we have in a dataframe."],"metadata":{}},{"cell_type":"code","source":["df.count()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["#### DataFrame structure\n\nTo get information about the schema associated with our dataframe we can use `printSchema`:"],"metadata":{}},{"cell_type":"code","source":["df.printSchema"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### show(..) vs display(..)\n* `show(..)` is part of core spark - `display(..)` is specific to our notebooks.\n* `show(..)` has parameters for truncating both columns and rows - `display(..)` does not.\n* `show(..)` is a function of the `DataFrame`/`Dataset` class - `display(..)` works with a number of different objects.\n* `display(..)` is more powerful - with it, you can...\n  * Download the results as CSV\n  * Render line charts, bar chart & other graphs, maps and more.\n  * See up to 1000 records at a time.\n  \nFor the most part, the difference between the two is going to come down to preference.\n\nRemember, the `display` function is Databricks specific. It is not available in standard spark code."],"metadata":{}},{"cell_type":"markdown","source":["## Querying dataframes\n\nOnce that spark has the data, we can manipulate it using spark SQL API.\n\nWe can easily use the spark SQL dsl to do joins, aggregations, filtering. \nWe can change the data structure, add or drop columns, or change the column types."],"metadata":{}},{"cell_type":"markdown","source":["We will use the python function we've already defined to convert Celsius degrees to Fahrenheit degrees."],"metadata":{}},{"cell_type":"code","source":["def celsiusToFahrenheit(source_temp=None):\n    return(source_temp * (9.0/5.0)) + 32.0\n  \ncelsiusToFahrenheit(27)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["We will adapt it as a udf (user defined function) to make it usable with Spark's dataframes API.\n\nAnd we will use it to enrich our source data."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import isnan, when, count, col\nfrom pyspark.sql.types import *\n\nudfCelsiusToFahrenheit = udf(lambda z: celsiusToFahrenheit(z), DoubleType())\n\ndisplay(df.filter(col('temperature').isNotNull()) \\\n  .withColumn(\"tempC\", col(\"temperature\").cast(DoubleType())) \\\n  .select(col(\"tempC\"), udfCelsiusToFahrenheit(col(\"tempC\")).alias(\"tempF\")))\n  "],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["More complex SQL functions are available in spark: \n\n* grouping, sorting, limits, count\n* aggregations: agg, max, sum\n* windowing: partitionBy, count over, max over\n\nFor example may want to add a row-number column to our source data. Window functions will help with such complex queries:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql.functions import desc, row_number, monotonically_increasing_id\n\ndisplay(df.orderBy('tripDistance', ascending=False) \\\n  .withColumn('rowno', row_number().over(Window.orderBy(monotonically_increasing_id()))))"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["#### Data cleaning\n\nBefore using the source data, we have to validate the contents. Let's see if there are any duplicates:"],"metadata":{}},{"cell_type":"code","source":["df.count() - df.dropDuplicates().count()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["Some columns might be missing. We check the presence of null values for each column."],"metadata":{}},{"cell_type":"code","source":["display(df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]))"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["Since some of our columns seem to have such null values, we'll have to fix these rows.\n\nWe could either replace null values using `fillna` or ignore such rows using `dropna`"],"metadata":{}},{"cell_type":"code","source":["df = df.fillna({'passengerCount':'1'}).dropna()\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["#### Explore Summary Statistics and Data Distribution\nPredictive modeling is based on statistics and probability, so we should take a look at the summary statistics for the columns in our data. The **describe** function returns a dataframe containing the **count**, **mean**, **standard deviation**, **minimum**, and **maximum** values for each numeric column."],"metadata":{}},{"cell_type":"code","source":["display(df.describe())\n"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["## Visualizing data\n\nAzure Databricks has custom support for displaying data. \n\nThe `display(..)` command has multiple capabilities:\n* Presents up to 1000 records.\n* Exporting data as CSV.\n* Rendering a multitude of different graphs.\n* Rendering geo-located data on a world map."],"metadata":{}},{"cell_type":"markdown","source":["Let's take a look at our data using databricks visualizations:\n* Run the cell below\n* click on the second icon underneath the executed cell and choose `Bar`\n* click on the `Plot Options` button to configure the graph\n  * drag the `tripDistance` into the `Keys` list\n  * drag the `totalAmount` into the `Values` list\n  * choose `Aggregation` as `AVG`\n  * click `Apply`"],"metadata":{}},{"cell_type":"code","source":["dfClean = df.select(col(\"tripDistance\"), col(\"totalAmount\")).dropna()\n\ndisplay(dfClean)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["Note that the points form a diagonal line, which indicates a strong linear relationship between the trip distance and the total amount. This linear relationship shows a correlation between these two values, which we can measure statistically. \n\nThe `corr` function calculates a correlation value between -1 and 1, indicating the strength of correlation between two fields. A strong positive correlation (near 1) indicates that high values for one column are often found with high values for the other, which a strong negative correlation (near -1) indicates that low values for one column are often found with high values for the other. A correlation near 0 indicates little apparent relationship between the fields."],"metadata":{}},{"cell_type":"code","source":["dfClean.corr('tripDistance', 'totalAmount')"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["Predictive modeling is largely based on statistical relationships between fields in the data. To design a good model, you need to understand how the data points relate to one another.\n\nA common way to start exploring relationships is to create visualizations that compare two or more data values. For example, modify the Plot Options of the chart above to compare the arrival delays for each carrier:\n\n* Keys: temperature\n* Series Groupings: month_num\n* Values: snowDeprh\n* Aggregation: avg\n* Display Type: Line Chart"],"metadata":{}},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["The plot now shows the relation between the month, the snow amount and the recorded temperature."],"metadata":{}}],"metadata":{"name":"Working with data in Azure Databricks","notebookId":836836672563037},"nbformat":4,"nbformat_minor":0}