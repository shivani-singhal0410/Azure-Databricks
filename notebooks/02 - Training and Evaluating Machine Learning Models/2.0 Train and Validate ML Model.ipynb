{"cells":[{"cell_type":"markdown","source":["# Training and Validating a Machine Learning Model\n\nLinear regression is the most commonly employed machine learning model since it is highly interpretable and well studied.  This is often the first pass for data scientists modeling continuous variables.  This notebook trains a multivariate regression model and interprets the results. This notebook is organized in two sections:\n\n- Exercise 1: Training a Model\n- Exercise 2: Validating a Model"],"metadata":{}},{"cell_type":"markdown","source":["Run the following cell to load common libraries."],"metadata":{}},{"cell_type":"code","source":["import urllib.request\nimport os\nimport numpy as np\nfrom pyspark.sql.types import * \nfrom pyspark.sql.functions import col, lit\nfrom pyspark.sql.functions import udf\nimport matplotlib\nimport matplotlib.pyplot as plt\nprint(\"Imported common libraries.\")"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Imported common libraries.\n</div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["## Load the training data\n\nIn this notebook, we will be using a subset of NYC Taxi & Limousine Commission - green taxi trip records available from [Azure Open Datasets]( https://azure.microsoft.com/en-us/services/open-datasets/). The data is enriched with holiday and weather data. Each row of the table represents a taxi ride that includes columns such as number of passengers, trip distance, datetime information, holiday and weather information, and the taxi fare for the trip.\n\nRun the following cell to load the table into a Spark dataframe and reivew the dataframe."],"metadata":{}},{"cell_type":"code","source":["dataset = spark.sql(\"select * from nyc_taxi\")\ndisplay(dataset)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## Exercise 1: Training a Model\n\nIn this section we will use the Spark's machine learning library, `MLlib` to train a `NYC Taxi Fare Predictor` machine learning model. We will train a multivariate regression model to predict taxi fares in New York City based on input features such as, number of passengers, trip distance, datetime, holiday information and weather information. Before we start, let's review the three main abstractions that are provided in the `MLlib`:<br><br>\n\n1. A **transformer** takes a DataFrame as an input and returns a new DataFrame with one or more columns appended to it.  \n  - Transformers implement a `.transform()` method.  \n2. An **estimator** takes a DataFrame as an input and returns a model, which itself is a transformer.\n  - Estimators implements a `.fit()` method.\n3. A **pipeline** combines together transformers and estimators to make it easier to combine multiple algorithms.\n  - Pipelines implement a `.fit()` method.\n  \nThese basic building blocks form the machine learning process in Spark from featurization through model training and deployment."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n\n### Featurization of the training data\n\nMachine learning models are only as strong as the data they see and can only work on numerical data.  **Featurization is the process of creating this input data for a model.** In this section we will build derived features and create a pipeline of featurization steps."],"metadata":{}},{"cell_type":"markdown","source":["Run the following cell to engineer the cyclical features to represent `hour_of_day`. Also, we will drop rows with null values in the `totalAmount` column and convert the column ` isPaidTimeOff ` as integer type."],"metadata":{}},{"cell_type":"code","source":["def get_sin_cosine(value, max_value):\n  sine =  np.sin(value * (2.*np.pi/max_value))\n  cosine = np.cos(value * (2.*np.pi/max_value))\n  return (sine.tolist(), cosine.tolist())\n\nschema = StructType([\n    StructField(\"sine\", DoubleType(), False),\n    StructField(\"cosine\", DoubleType(), False)\n])\n\nget_sin_cosineUDF = udf(get_sin_cosine, schema)\n\ndataset = dataset.withColumn(\"udfResult\", get_sin_cosineUDF(col(\"hour_of_day\"), lit(24))).withColumn(\"hour_sine\", col(\"udfResult.sine\")).withColumn(\"hour_cosine\", col(\"udfResult.cosine\")).drop(\"udfResult\").drop(\"hour_of_day\")\n\ndataset = dataset.filter(dataset.totalAmount.isNotNull())\n\ndataset = dataset.withColumn(\"isPaidTimeOff\", col(\"isPaidTimeOff\").cast(\"integer\"))\n\ndisplay(dataset)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Run the following cell to create stages in our featurization pipeline to scale the numerical features and to encode the categorical features."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import Imputer\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import MinMaxScaler\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml import Pipeline\n\nnumerical_cols = [\"passengerCount\", \"tripDistance\", \"snowDepth\", \"precipTime\", \"precipDepth\", \"temperature\", \"hour_sine\", \"hour_cosine\"]\ncategorical_cols = [\"day_of_week\", \"month_num\", \"normalizeHolidayName\", \"isPaidTimeOff\"]\nlabel_column = \"totalAmount\"\n\nstages = []\n\ninputCols = [\"passengerCount\"]\noutputCols = [\"passengerCount\"]\nimputer = Imputer(strategy=\"median\", inputCols=inputCols, outputCols=outputCols)\nstages += [imputer]\n\nassembler = VectorAssembler().setInputCols(numerical_cols).setOutputCol('numerical_features')\nscaler = MinMaxScaler(inputCol=assembler.getOutputCol(), outputCol=\"scaled_numerical_features\")\nstages += [assembler, scaler]\n\nfor categorical_col in categorical_cols:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categorical_col, outputCol=categorical_col + \"_index\", handleInvalid=\"skip\")\n    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categorical_col + \"_classVector\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n    \nprint(\"Created stages in our featurization pipeline to scale the numerical features and to encode the categorical features.\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Use a `VectorAssembler` to combine all the feature columns into a single vector column named **features**."],"metadata":{}},{"cell_type":"code","source":["assemblerInputs = [c + \"_classVector\" for c in categorical_cols] + [\"scaled_numerical_features\"]\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]\nprint(\"Used a VectorAssembler to combine all the feature columns into a single vector column named features.\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["-sandbox\n**Run the stages as a Pipeline**\n\nThe pipeline is itself is now an `estimator`.  Call the pipeline's `fit` method and then `transform` the original dataset. This puts the data through all of the feature transformations we described in a single call. Observe the new columns, especially column: **features**."],"metadata":{}},{"cell_type":"code","source":["partialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n\ndisplay(preppedDataDF)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["-sandbox\n\n### Train a multivariate regression model\n\nA multivariate regression takes an arbitrary number of input features. The equation for multivariate regression looks like the following where each feature `p` has its own coefficient:\n\n&nbsp;&nbsp;&nbsp;&nbsp;`Y ≈ β<sub>0</sub> + β<sub>1</sub>X<sub>1</sub> + β<sub>2</sub>X<sub>2</sub> + ... + β<sub>p</sub>X<sub>p</sub>`"],"metadata":{}},{"cell_type":"markdown","source":["Split the featurized training data for training and validating the model"],"metadata":{}},{"cell_type":"code","source":["(trainingData, testData) = preppedDataDF.randomSplit([0.7, 0.3], seed=97)\nprint(\"The training data is split for training and validating the model: 70-30 split.\")"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Create the estimator `LinearRegression` and call its `fit` method to get back the trained ML model (`lrModel`). You can read more about [Linear Regression] from the [classification and regression] section of MLlib Programming Guide.\n\n[classification and regression]: https://spark.apache.org/docs/latest/ml-classification-regression.html\n[Linear Regression]: https://spark.apache.org/docs/3.1.1/ml-classification-regression.html#linear-regression"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import LinearRegression\n\nlr = LinearRegression(featuresCol=\"features\", labelCol=label_column)\n\nlrModel = lr.fit(trainingData)\n\nprint(lrModel)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["## Exercise 2: Validating a Model"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n\nFrom the trained model summary, let’s review some of the model performance metrics such as, Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R<sup>2</sup> score. We will also look at the multivariate model’s coefficients."],"metadata":{}},{"cell_type":"code","source":["summary = lrModel.summary\nprint(\"RMSE score: {} \\nMAE score: {} \\nR2 score: {}\".format(summary.rootMeanSquaredError, summary.meanAbsoluteError, lrModel.summary.r2))\nprint(\"\")\nprint(\"β0 (intercept): {}\".format(lrModel.intercept))\ni = 0\nfor coef in lrModel.coefficients:\n  i += 1\n  print(\"β{} (coefficient): {}\".format(i, coef))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["-sandbox\n\nEvaluate the model performance using the hold-back  dataset. Observe that the RMSE and R<sup>2</sup> score on holdback dataset is slightly degraded compared to the training summary. A big disparity in performance metrics between training and hold-back dataset can be an indication of model overfitting the training data."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n\npredictions = lrModel.transform(testData)\nevaluator = RegressionEvaluator(\n    labelCol=label_column, predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\nevaluator = RegressionEvaluator(\n    labelCol=label_column, predictionCol=\"prediction\", metricName=\"mae\")\nmae = evaluator.evaluate(predictions)\nprint(\"MAE on test data = %g\" % mae)\nevaluator = RegressionEvaluator(\n    labelCol=label_column, predictionCol=\"prediction\", metricName=\"r2\")\nr2 = evaluator.evaluate(predictions)\nprint(\"R2 on test data = %g\" % r2)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["**Compare the summary statistics between the true values and the model predictions**"],"metadata":{}},{"cell_type":"code","source":["display(predictions.select([\"totalAmount\",  \"prediction\"]).describe())"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["**Visualize the plot between true values and the model predictions**"],"metadata":{}},{"cell_type":"code","source":["p_df = predictions.select([\"totalAmount\",  \"prediction\"]).toPandas()\ntrue_value = p_df.totalAmount\npredicted_value = p_df.prediction\n\nplt.figure(figsize=(10,10))\nplt.scatter(true_value, predicted_value, c='crimson')\nplt.yscale('log')\nplt.xscale('log')\n\np1 = max(max(predicted_value), max(true_value))\np2 = min(min(predicted_value), min(true_value))\nplt.plot([p1, p2], [p1, p2], 'b-')\nplt.xlabel('True Values', fontsize=15)\nplt.ylabel('Predictions', fontsize=15)\nplt.axis('equal')\nplt.show()"],"metadata":{},"outputs":[],"execution_count":29}],"metadata":{"name":"2.0 Train and Validate ML Model","notebookId":836836672563077},"nbformat":4,"nbformat_minor":0}